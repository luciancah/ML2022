{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing numpy \nimport numpy as np\n# Importing Scipy \nimport scipy as sp\n# Importing Pandas Library \nimport pandas as pd\n# import glob function to scrap files path\nfrom glob import glob\n# import display() for better visualitions of DataFrames and arrays\nfrom IPython.display import display\n# import pyplot for plotting\nimport matplotlib.pyplot as plt\nplt.style.use('bmh') # for better plots\nimport tqdm\n\n# import data_loader for data loading\nfrom data_loader import import_raw_signals, import_labels_file,normalize5,normalize2","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:33:48.169045Z","iopub.execute_input":"2022-04-24T13:33:48.169753Z","iopub.status.idle":"2022-04-24T13:33:48.239142Z","shell.execute_reply.started":"2022-04-24T13:33:48.169647Z","shell.execute_reply":"2022-04-24T13:33:48.238476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Raw_data_paths = sorted(glob(\"../input/2022-ml-w7p1/RawData/*\"))\nRaw_acc_paths=Raw_data_paths[0:61]\nRaw_gyro_paths=Raw_data_paths[61:122]\n\nprint ((\"RawData folder contains in total {:d} file \").format(len(Raw_data_paths)))\nprint ((\"The first {:d} are Acceleration files:\").format(len(Raw_acc_paths)))\nprint ((\"The second {:d} are Gyroscope files:\").format(len(Raw_gyro_paths)))\nprint (\"The last file is a labels file\")\nprint (\"test labels file path is:\",Raw_data_paths[122])\nprint (\"train labels file path is:\",Raw_data_paths[123])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:33:48.241444Z","iopub.execute_input":"2022-04-24T13:33:48.241692Z","iopub.status.idle":"2022-04-24T13:33:48.269031Z","shell.execute_reply.started":"2022-04-24T13:33:48.241658Z","shell.execute_reply":"2022-04-24T13:33:48.268377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_dic={}\nraw_acc_columns=['acc_X','acc_Y','acc_Z']\nraw_gyro_columns=['gyro_X','gyro_Y','gyro_Z']\nfor path_index in range(0,61):\n        key= Raw_data_paths[path_index][-16:-4]\n        raw_acc_data_frame=import_raw_signals(Raw_data_paths[path_index],raw_acc_columns)\n        raw_gyro_data_frame=import_raw_signals(Raw_data_paths[path_index+61],raw_gyro_columns)\n        raw_signals_data_frame=pd.concat([raw_acc_data_frame, raw_gyro_data_frame], axis=1)\n        raw_dic[key]=raw_signals_data_frame","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:33:48.270902Z","iopub.execute_input":"2022-04-24T13:33:48.272336Z","iopub.status.idle":"2022-04-24T13:34:02.387161Z","shell.execute_reply.started":"2022-04-24T13:33:48.272295Z","shell.execute_reply":"2022-04-24T13:34:02.386421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('raw_dic contains %d DataFrame' % len(raw_dic))\ndisplay(raw_dic['exp01_user01'].head(3))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:34:02.389087Z","iopub.execute_input":"2022-04-24T13:34:02.389314Z","iopub.status.idle":"2022-04-24T13:34:02.409301Z","shell.execute_reply.started":"2022-04-24T13:34:02.389288Z","shell.execute_reply":"2022-04-24T13:34:02.40853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_raw_labels_columns=['experiment_number_ID','user_number_ID','activity_number_ID','Label_start_point','Label_end_point']\ntest_raw_labels_columns=['experiment_number_ID','user_number_ID','Label_start_point','Label_end_point']\n\ntest_labels_path=Raw_data_paths[122]\ntrain_labels_path=Raw_data_paths[123]\n\ntrain_Labels_Data_Frame=import_labels_file(train_labels_path,train_raw_labels_columns)\ntest_Labels_Data_Frame=import_labels_file(test_labels_path,test_raw_labels_columns)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:34:02.41055Z","iopub.execute_input":"2022-04-24T13:34:02.410864Z","iopub.status.idle":"2022-04-24T13:34:02.424919Z","shell.execute_reply.started":"2022-04-24T13:34:02.410828Z","shell.execute_reply":"2022-04-24T13:34:02.424157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"The first 3 rows of  train_Labels_Data_Frame:\" )\ndisplay(train_Labels_Data_Frame.head(3))\nprint(train_Labels_Data_Frame.shape)\ndisplay(test_Labels_Data_Frame.head(3))\nprint(test_Labels_Data_Frame.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:34:02.427465Z","iopub.execute_input":"2022-04-24T13:34:02.427647Z","iopub.status.idle":"2022-04-24T13:34:02.44679Z","shell.execute_reply.started":"2022-04-24T13:34:02.427624Z","shell.execute_reply":"2022-04-24T13:34:02.446097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.signal import medfilt\n\ndef median(signal):\n    array=np.array(signal)   \n    med_filtered=sp.signal.medfilt(array, kernel_size=3)\n    return  med_filtered","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:34:02.447955Z","iopub.execute_input":"2022-04-24T13:34:02.44828Z","iopub.status.idle":"2022-04-24T13:34:03.169344Z","shell.execute_reply.started":"2022-04-24T13:34:02.448238Z","shell.execute_reply":"2022-04-24T13:34:03.168552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.fftpack import fft  \nfrom scipy.fftpack import fftfreq\nfrom scipy.fftpack import ifft\nimport math \n\nsampling_freq = 50\nnyq=sampling_freq/float(2)\nfreq1 = 0.3\nfreq2 = 20\n\n# Function name: components_selection_one_signal\n\n# Inputs: t_signal:1D numpy array (time domain signal); \n\n# Outputs: (total_component,t_DC_component , t_body_component, t_noise) \n#           type(1D array,1D array, 1D array)\n\n# cases to discuss: if the t_signal is an acceleration signal then the t_DC_component is the gravity component [Grav_acc]\n#                   if the t_signal is a gyro signal then the t_DC_component is not useful\n# t_noise component is not useful\n# if the t_signal is an acceleration signal then the t_body_component is the body's acceleration component [Body_acc]\n# if the t_signal is a gyro signal then the t_body_component is the body's angular velocity component [Body_gyro]\n\ndef components_selection_one_signal(t_signal,freq1,freq2):\n    t_signal=np.array(t_signal)\n    t_signal_length=len(t_signal)\n    f_signal=fft(t_signal)\n    freqs=np.array(sp.fftpack.fftfreq(t_signal_length, d=1/float(sampling_freq)))# frequency values between [-25hz:+25hz]\n    \n    # DC_component: f_signal values having freq between [-0.3 hz to 0 hz] and from [0 hz to 0.3hz] \n    #                                                             (-0.3 and 0.3 are included)\n    \n    # noise components: f_signal values having freq between [-25 hz to 20 hz[ and from ] 20 hz to 25 hz] \n    #                                                               (-25 and 25 hz inculded 20hz and -20hz not included)\n    \n    # selecting body_component: f_signal values having freq between [-20 hz to -0.3 hz] and from [0.3 hz to 20 hz] \n    #                                                               (-0.3 and 0.3 not included , -20hz and 20 hz included)\n    \n    \n    f_DC_signal=[] # DC_component in freq domain\n    f_body_signal=[] # body component in freq domain numpy.append(a, a[0])\n    f_noise_signal=[] # noise in freq domain\n    \n    for i in range(len(freqs)):# iterate over all available frequencies\n        \n        # selecting the frequency value\n        freq=freqs[i]\n        \n        # selecting the f_signal value associated to freq\n        value= f_signal[i]\n        \n        # Selecting DC_component values \n        if abs(freq)>0.3:# testing if freq is outside DC_component frequency ranges\n            f_DC_signal.append(float(0)) # add 0 to  the  list if it was the case (the value should not be added)                                       \n        else: # if freq is inside DC_component frequency ranges \n            f_DC_signal.append(value) # add f_signal value to f_DC_signal list\n    \n        # Selecting noise component values \n        if (abs(freq)<=20):# testing if freq is outside noise frequency ranges \n            f_noise_signal.append(float(0)) # # add 0 to  f_noise_signal list if it was the case \n        else:# if freq is inside noise frequency ranges \n            f_noise_signal.append(value) # add f_signal value to f_noise_signal\n\n        # Selecting body_component values \n        if (abs(freq)<=0.3 or abs(freq)>20):# testing if freq is outside Body_component frequency ranges\n            f_body_signal.append(float(0))# add 0 to  f_body_signal list\n        else:# if freq is inside Body_component frequency ranges\n            f_body_signal.append(value) # add f_signal value to f_body_signal list\n    \n    ################### Inverse the transformation of signals in freq domain ########################\n    # applying the inverse fft(ifft) to signals in freq domain and put them in float format\n    t_DC_component= ifft(np.array(f_DC_signal)).real\n    t_body_component= ifft(np.array(f_body_signal)).real\n    t_noise=ifft(np.array(f_noise_signal)).real\n    \n    total_component=t_signal-t_noise # extracting the total component(filtered from noise) \n                                     #  by substracting noise from t_signal (the original signal).\n    \n    # return outputs mentioned earlier\n    return (total_component,t_DC_component,t_body_component,t_noise) ","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:34:03.170784Z","iopub.execute_input":"2022-04-24T13:34:03.171186Z","iopub.status.idle":"2022-04-24T13:34:03.197483Z","shell.execute_reply.started":"2022-04-24T13:34:03.171147Z","shell.execute_reply":"2022-04-24T13:34:03.196753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\ndef mag_3_signals(x,y,z): # Euclidian magnitude\n    return [math.sqrt((x[i]**2+y[i]**2+z[i]**2)) for i in range(len(x))]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:34:03.198663Z","iopub.execute_input":"2022-04-24T13:34:03.199011Z","iopub.status.idle":"2022-04-24T13:34:03.204352Z","shell.execute_reply.started":"2022-04-24T13:34:03.198977Z","shell.execute_reply":"2022-04-24T13:34:03.203452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt=0.02 # dt=1/50=0.02s time duration between two rows\n# Input: 1D array with lenght=N (N:unknown)\n# Output: 1D array with lenght=N-1\ndef jerk_one_signal(signal): \n        return np.array([(signal[i+1]-signal[i])/dt for i in range(len(signal)-1)])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:34:03.211473Z","iopub.execute_input":"2022-04-24T13:34:03.212118Z","iopub.status.idle":"2022-04-24T13:34:03.221177Z","shell.execute_reply.started":"2022-04-24T13:34:03.212079Z","shell.execute_reply":"2022-04-24T13:34:03.220256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_sig_dic={}\nraw_dic_keys=sorted(raw_dic.keys())\n\nfor key in tqdm.tqdm(raw_dic_keys):\n    raw_df=raw_dic[key]\n    time_sig_df=pd.DataFrame()\n    \n    for column in raw_df.columns:\n        t_signal=np.array(raw_df[column])\n        med_filtred=median(t_signal)\n        \n        if 'acc' in column:\n            _,grav_acc,body_acc,_=components_selection_one_signal(med_filtred,freq1,freq2)\n            body_acc_jerk=jerk_one_signal(body_acc)\n            time_sig_df['t_body_'+column]=body_acc[:-1]\n            time_sig_df['t_grav_'+column]= grav_acc[:-1]\n            time_sig_df['t_body_acc_jerk_'+column[-1]]=body_acc_jerk\n        elif 'gyro' in column:\n            _,_,body_gyro,_=components_selection_one_signal(med_filtred,freq1,freq2)\n            body_gyro_jerk=jerk_one_signal(body_gyro)\n            time_sig_df['t_body_gyro_'+column[-1]]=body_gyro[:-1]\n            time_sig_df['t_body_gyro_jerk_'+column[-1]]=body_gyro_jerk\n            \n    new_columns_ordered=['t_body_acc_X','t_body_acc_Y','t_body_acc_Z',\n                      't_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z',\n                      't_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z',\n                      't_body_gyro_X','t_body_gyro_Y','t_body_gyro_Z',\n                      't_body_gyro_jerk_X','t_body_gyro_jerk_Y','t_body_gyro_jerk_Z']\n        \n    ordered_time_sig_df=pd.DataFrame()\n        \n    for col in new_columns_ordered:\n        ordered_time_sig_df[col]=time_sig_df[col]\n        \n    for i in range(0,15,3):\n        mag_col_name=new_columns_ordered[i][:-1]+'mag'\n        col0=np.array(ordered_time_sig_df[new_columns_ordered[i]]) # copy X_component\n        col1=ordered_time_sig_df[new_columns_ordered[i+1]] # copy Y_component\n        col2=ordered_time_sig_df[new_columns_ordered[i+2]] # copy Z_component\n        mag_signal=mag_3_signals(col0,col1,col2)\n        ordered_time_sig_df[mag_col_name]=mag_signal\n        \n    time_sig_dic[key]=ordered_time_sig_df","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:34:03.225753Z","iopub.execute_input":"2022-04-24T13:34:03.228016Z","iopub.status.idle":"2022-04-24T13:35:25.401891Z","shell.execute_reply.started":"2022-04-24T13:34:03.227978Z","shell.execute_reply":"2022-04-24T13:35:25.40122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(time_sig_dic['exp01_user01'].shape)\ndisplay(time_sig_dic['exp01_user01'].describe())\ntime_sig_dic['exp01_user01'].head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:35:25.403114Z","iopub.execute_input":"2022-04-24T13:35:25.403752Z","iopub.status.idle":"2022-04-24T13:35:25.49415Z","shell.execute_reply.started":"2022-04-24T13:35:25.403711Z","shell.execute_reply":"2022-04-24T13:35:25.493374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Windowing_type(time_sig_dic,Labels_Data_Frame):\n#     import pdb;pdb.set_trace()\n    columns=time_sig_dic['exp01_user01'].columns\n    window_ID=0\n    time_dictionary_window={}\n    BA_array=np.array(Labels_Data_Frame)\n    \n    for line in tqdm.tqdm(BA_array):\n        file_key= 'exp' + normalize2(int(line[0]))  +  '_user' + normalize2(int(line[1]))\n        \n        if line.shape[0] == 5 :\n          act_ID=line[2]\n          start_point=line[3]\n          end_point = line[4]\n        else :\n          act_ID='None'\n          start_point = line[2]\n          end_point = line[3]\n        \n        for cursor in range(start_point,end_point-127,64):\n            end_point=cursor+128\n            data=np.array(time_sig_dic[file_key].iloc[cursor:end_point])\n            window=pd.DataFrame(data=data,columns=columns)\n            key='t_W'+normalize5(window_ID)+'_'+file_key+'_act'+normalize2(act_ID)\n            time_dictionary_window[key]=window\n            window_ID=window_ID+1\n    \n    return time_dictionary_window ","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:35:25.495392Z","iopub.execute_input":"2022-04-24T13:35:25.495714Z","iopub.status.idle":"2022-04-24T13:35:25.504739Z","shell.execute_reply.started":"2022-04-24T13:35:25.495677Z","shell.execute_reply":"2022-04-24T13:35:25.503977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_time_dictionary_window  = Windowing_type(time_sig_dic,train_Labels_Data_Frame)\ntest_time_dictionary_window  = Windowing_type(time_sig_dic,test_Labels_Data_Frame)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:35:25.506139Z","iopub.execute_input":"2022-04-24T13:35:25.506856Z","iopub.status.idle":"2022-04-24T13:35:30.073067Z","shell.execute_reply.started":"2022-04-24T13:35:25.506754Z","shell.execute_reply":"2022-04-24T13:35:30.072261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_window = train_time_dictionary_window[sorted(train_time_dictionary_window.keys())[0]]\ntrain_window.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:35:30.074416Z","iopub.execute_input":"2022-04-24T13:35:30.074679Z","iopub.status.idle":"2022-04-24T13:35:30.099804Z","shell.execute_reply.started":"2022-04-24T13:35:30.074644Z","shell.execute_reply":"2022-04-24T13:35:30.099083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"시간 도메인 Train 데이터 수 : {}\".format(len(train_time_dictionary_window)))\nprint(\"시간 도메인 Test 데이터 수 : {}\".format(len(test_time_dictionary_window)))\nprint(\"윈도우 크기(2.56s => 128개) : {}\".format(len(train_window)))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:35:30.101355Z","iopub.execute_input":"2022-04-24T13:35:30.102066Z","iopub.status.idle":"2022-04-24T13:35:30.108672Z","shell.execute_reply.started":"2022-04-24T13:35:30.102009Z","shell.execute_reply":"2022-04-24T13:35:30.107891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import fftpack\nfrom numpy.fft import *\n\ndef fast_fourier_transform_one_signal(t_signal):\n    complex_f_signal= fftpack.fft(t_signal)\n    amplitude_f_signal=np.abs(complex_f_signal)\n    \n    return amplitude_f_signal\n\ndef fast_fourier_transform(t_window):\n    f_window=pd.DataFrame()\n    for column in t_window.columns:\n        if 'grav' not in column:\n            t_signal=np.array(t_window[column])\n            f_signal= np.apply_along_axis(fast_fourier_transform_one_signal,0,t_signal)\n            f_window[\"f_\"+column[2:]]=f_signal\n    return f_window","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:35:30.110249Z","iopub.execute_input":"2022-04-24T13:35:30.110798Z","iopub.status.idle":"2022-04-24T13:35:30.117794Z","shell.execute_reply.started":"2022-04-24T13:35:30.110763Z","shell.execute_reply":"2022-04-24T13:35:30.117087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_frequent_dictionary_window = {'f'+key[1:] : train_t_df.pipe(fast_fourier_transform) for key, train_t_df in tqdm.tqdm(train_time_dictionary_window.items())}\ntest_frequent_dictionary_window = {'f'+key[1:] : test_t_df.pipe(fast_fourier_transform) for key, test_t_df in tqdm.tqdm(test_time_dictionary_window.items())}","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:35:30.12038Z","iopub.execute_input":"2022-04-24T13:35:30.121119Z","iopub.status.idle":"2022-04-24T13:37:00.636291Z","shell.execute_reply.started":"2022-04-24T13:35:30.121082Z","shell.execute_reply":"2022-04-24T13:37:00.635583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_window = train_frequent_dictionary_window[sorted(train_frequent_dictionary_window.keys())[0]]\ntrain_window.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:37:00.637292Z","iopub.execute_input":"2022-04-24T13:37:00.637554Z","iopub.status.idle":"2022-04-24T13:37:00.655743Z","shell.execute_reply.started":"2022-04-24T13:37:00.637507Z","shell.execute_reply":"2022-04-24T13:37:00.65507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"주파수 도메인 Train 데이터 수 : {}\".format(len(train_frequent_dictionary_window)))\nprint(\"주파수 도메인 Test 데이터 수 : {}\".format(len(test_frequent_dictionary_window)))\nprint(\"피처의 갯수 : {}\".format(len(train_window)))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:37:00.656847Z","iopub.execute_input":"2022-04-24T13:37:00.6571Z","iopub.status.idle":"2022-04-24T13:37:00.67171Z","shell.execute_reply.started":"2022-04-24T13:37:00.657065Z","shell.execute_reply":"2022-04-24T13:37:00.670756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------------------------\n# [Empty Module #1] Feature Engineering\n# -------------------------------------\n\n# -------------------------------------\n# Feature Engineering\n# -------------------------------------\n# 목적: 제공된 36개의 시퀀스 도메인 데이터를 기반으로 유의미한 피처를 추출한다.\n# 입력인자: 시간(time) 도메인 Feature 20개 , 주파수(frequency) 도메인 Feature 16개\n# 출력인자: 분류모델 학습을 위한 Feature\n# -------------------------------------\n\n# ------------------------------------------------------------\n# 구현 가이드라인 - 논문에서 제안하는 Feature Engineering 방법\n# ------------------------------------------------------------\n#\n# mean(): Mean value\n# std(): Standard deviation\n# mad(): Median absolute deviation \n# max(): Largest value in array\n# min(): Smallest value in array\n# sma(): Signal magnitude area\n# energy(): Energy measure. Sum of the squares divided by the number of values. \n# iqr(): Interquartile range \n# entropy(): Signal entropy\n# arCoeff(): Autorregresion coefficients with Burg order equal to 4\n# correlation(): correlation coefficient between two signals\n# maxInds(): index of the frequency component with largest magnitude\n# meanFreq(): Weighted average of the frequency components to obtain a mean frequency\n# skewness(): skewness of the frequency domain signal \n# kurtosis(): kurtosis of the frequency domain signal \n# bandsEnergy(): Energy of a frequency interval within the 64 bins of the FFT of each window.\n# angle(): Angle between to vectors.\n\nimport sys\n\n\n# Time domain Feature Extract function\n\nfrom Feature_engineering import mean_axial,std_axial,mad_axial,max_axial,min_axial, t_sma_axial, t_energy_axial,IQR_axial,entropy_axial, t_arburg_axial, t_corr_axial\nfrom Feature_engineering import mean_mag,std_mag,mad_mag,max_mag,min_mag,t_sma_mag,t_energy_mag,IQR_mag,entropy_mag,t_arburg_mag\n\n# Frequency domain Feature Extract function\nfrom Feature_engineering import f_sma_axial,f_energy_axial,f_max_Inds_axial,f_mean_Freq_axial,f_skewness_and_kurtosis_axial,f_all_bands_energy_axial\nfrom Feature_engineering import f_sma_mag,f_energy_mag,f_max_Inds_mag,f_mean_Freq_mag,f_skewness_mag,f_kurtosis_mag\n\n# Additional Feature Extract function\nfrom Feature_engineering import angle_features\n\ndef feature_extractor(time_dictionary,freq_dictionary, condition='train') :\n    \n    \n    if condition is 'train' :\n        total_data = []\n        total_label = []\n    elif condition is 'test' :\n        total_data = []\n        \n    for i in tqdm.tqdm(range(len(time_dictionary))) :\n        \n        time_key = sorted(time_dictionary.keys())[i]\n        freq_key = sorted(freq_dictionary.keys())[i]\n        \n        time_window = time_dictionary[time_key]\n        freq_window = freq_dictionary[freq_key]\n        \n        if condition is 'train' :\n          window_user_id= int(time_key[-8:-6]) # extract the user id from window's key\n          window_activity_id=int(time_key[-2:]) # extract the activity id from the windows key\n        elif condition is 'test' :\n          window_user_id= int(time_key[-10:-8]) # extract the user id from window's key\n          window_activity_id= 0\n        else :\n            print(\"Error\")\n            sys.exit()\n            break;\n            \n        ##################################################################################\n        \n        \n        # Time domain - Feature extractor - Part 1. axial(X,Y,Z) Features \n        \n        #[0,1,2] : 't_body_acc_X', 't_body_acc_Y', 't_body_acc_Z'\n        #[3,4,5] : 't_grav_acc_X','t_grav_acc_Y', 't_grav_acc_Z'\n        #[6,7,8] : 't_body_acc_jerk_X','t_body_acc_jerk_Y', 't_body_acc_jerk_Z'\n        #[9,10,11] : 't_body_gyro_X','t_body_gyro_Y', 't_body_gyro_Z'\n        #[12,13,14] : 't_body_gyro_jerk_X', 't_body_gyro_jerk_Y', 't_body_gyro_jerk_Z'\n        \n        axial_columns = time_window.columns[0:15]\n        axial_df = time_window[axial_columns] # X,Y,Z\n        \n        time_axial_features = []\n        \n        for col in range(0,15,3) : \n            # ------------------------------------------------------------\n            # 구현 가이드라인 \n            # ------------------------------------------------------------\n            # 아래 time_3axial_vector 나타난 Feature를 계산하여야 한다.\n            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n            # ------------------------------------------------------------\n            curr_col = axial_columns[col:col+3]\n            curr_df = axial_df[curr_col]\n            \n            mean_vector = mean_axial(curr_df)\n            std_vector = std_axial(curr_df)\n            mad_vector = mad_axial(curr_df)\n            max_vector = max_axial(curr_df)\n            min_vector = min_axial(curr_df)\n            sma_value = t_sma_axial(curr_df)\n            energy_vector = t_energy_axial(curr_df)\n            IQR_vector = IQR_axial(curr_df)\n            entropy_vector = entropy_axial(curr_df)\n            AR_vector = t_arburg_axial(curr_df)\n            corr_vector = t_corr_axial(curr_df)\n            # 40 value per each 3-axial signals\n            time_3axial_vector = mean_vector + std_vector + mad_vector + \\\n                                 max_vector + min_vector + [sma_value] + \\\n                                 energy_vector + IQR_vector + entropy_vector + \\\n                                 AR_vector + corr_vector\n            \n            # append these features to the global list of features\n            time_axial_features= time_axial_features+time_3axial_vector\n        \n        ##################################################################################\n        \n        # Time domain - Feature extractor - Part 2. Magnitude Features \n        \n        #[15]'t_body_acc_mag'\n        #[16]'t_grav_acc_mag'\n        #[17]'t_body_acc_jerk_mag'\n        #[18]'t_body_gyro_mag'\n        #[19]'t_body_gyro_jerk_mag'\n        \n        mag_columns = time_window.columns[15:]\n        mag_columns = time_window[mag_columns]\n        \n        time_mag_features = []\n        \n        for ci, col in enumerate(mag_columns) :\n            \n            # ------------------------------------------------------------\n            # 구현 가이드라인 \n            # ------------------------------------------------------------\n            # 아래 col_mag_values 나타난 Feature를 계산하여야 한다.\n            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n            # ------------------------------------------------------------\n   \n            mean_value = mean_mag(mag_columns[col])\n            std_value = std_mag(mag_columns[col])\n            mad_value = mad_mag(mag_columns[col])\n            max_value = max_mag(mag_columns[col])\n            min_value = min_mag(mag_columns[col])\n            sma_value = f_sma_mag(mag_columns[col])\n            energy_value = t_energy_mag(mag_columns[col])\n            IQR_value = IQR_mag(mag_columns[col])\n            entropy_value = entropy_mag(mag_columns[col])\n            \n            curr_col = axial_columns[int(ci*3):int(ci*3)+3]\n            curr_df = axial_df[curr_col]\n            AR_vector = t_arburg_axial(curr_df)\n            \n            # 13 value per each t_mag_column\n            col_mag_values = [mean_value, std_value, mad_value, max_value, min_value, sma_value, \n                              energy_value,IQR_value, entropy_value]+ AR_vector\n\n            # col_mag_values will be added to the global list\n            time_mag_features= time_mag_features+ col_mag_values\n\n        \n        ##################################################################################\n        \n        # Frequency domain - Feature extractor - Part 1. axial(X,Y,Z) Features \n        \n        #[0,1,2] : 'f_body_acc_X', 'f_body_acc_Y', 'f_body_acc_Z'\n        #[3,4,5] : 'f_body_acc_jerk_X','f_body_acc_jerk_Y', 'f_body_acc_jerk_Z'\n        #[6,7,8] : 'f_body_gyro_X','f_body_gyro_Y', 'f_body_gyro_Z'\n        #[9,10,11] : 'f_body_gyro_jerk_X','f_body_gyro_jerk_Y', 'f_body_gyro_jerk_Z'\n        \n        axial_columns=freq_window.columns[0:12]\n        axial_df=freq_window[axial_columns]\n        freq_axial_features=[]\n        \n        for col in range(0,12,3) :         \n            # ------------------------------------------------------------\n            # 구현 가이드라인 \n            # ------------------------------------------------------------\n            # 아래 freq_3axial_features 나타난 Feature를 계산하여야 한다.\n            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n            # ------------------------------------------------------------\n            \n            curr_col = axial_columns[col:col+3]\n            curr_df = axial_df[curr_col]\n            \n            mean_vector = mean_axial(curr_df)\n            std_vector = std_axial(curr_df)\n            mad_vector = mad_axial(curr_df)\n            max_vector = max_axial(curr_df)\n            min_vector = min_axial(curr_df)\n            sma_value = f_sma_axial(curr_df)\n            energy_vector = f_energy_axial(curr_df)\n            IQR_vector = IQR_axial(curr_df)\n            entropy_vector = entropy_axial(curr_df)\n            max_inds_vector = f_max_Inds_axial(curr_df)\n            mean_Freq_vector = f_mean_Freq_axial(curr_df)\n            skewness_and_kurtosis_vector = f_skewness_and_kurtosis_axial(curr_df)\n            bands_energy_vector = f_all_bands_energy_axial(curr_df)\n            \n            freq_3axial_features = mean_vector +std_vector + mad_vector + max_vector + min_vector + [sma_value] + energy_vector + IQR_vector + entropy_vector + max_inds_vector + mean_Freq_vector + skewness_and_kurtosis_vector + bands_energy_vector\n            freq_axial_features = freq_axial_features+ freq_3axial_features\n        \n        ##################################################################################\n        \n        # Frequency domain - Feature extractor - Part 2. Magnitude Features\n        \n        #[12]'f_body_acc_mag'\n        #[13]'f_body_acc_jerk_mag'\n        #[14]'f_body_gyro_mag'\n        #[15]'f_body_gyro_jerk_mag'\n        \n        mag_columns=freq_window.columns[12:]\n        mag_columns=freq_window[mag_columns]\n        \n        freq_mag_features = []\n        \n        for col in mag_columns:\n            # ------------------------------------------------------------\n            # 구현 가이드라인 \n            # ------------------------------------------------------------\n            # 아래 col_mag_values에 나타난 Feature를 계산하여야 한다.\n            # 각각을 계산하기위한 함수는 'Feature_engineering.py'에 내제되어 있다.\n            # ------------------------------------------------------------\n            mean_value = mean_mag(mag_columns[col])\n            std_value = std_mag(mag_columns[col])\n            mad_value = mad_mag(mag_columns[col])\n            max_value = max_mag(mag_columns[col])\n            min_value = min_mag(mag_columns[col])\n            sma_value = t_sma_mag(mag_columns[col])\n            energy_value = t_energy_mag(mag_columns[col])\n            IQR_value = IQR_mag(mag_columns[col])\n            entropy_value = entropy_mag(mag_columns[col])\n            max_Inds_value = f_max_Inds_mag(mag_columns[col])\n            mean_Freq_value = f_mean_Freq_mag(mag_columns[col])\n            skewness_value = f_skewness_mag(mag_columns[col])\n            kurtosis_value = f_kurtosis_mag(mag_columns[col])\n            # 13 value per each t_mag_column\n            col_mag_values = [mean_value, std_value, mad_value, max_value, \n                              min_value, sma_value, energy_value,IQR_value, \n                              entropy_value, max_Inds_value, mean_Freq_value,\n                              skewness_value, kurtosis_value ]\n            \n            freq_mag_features= freq_mag_features+ col_mag_values\n        \n        ##################################################################################\n        \n        # Time domain - Feature extractor - Part 3. Additional Features \n        \n        additional_features = angle_features(time_window)\n                \n        ##################################################################################\n        \n        total_features = time_axial_features + time_mag_features + freq_axial_features + freq_mag_features + additional_features\n        \n        total_data.append(total_features)\n        if condition is 'train' :\n            total_label.append(window_activity_id)\n    \n    total_data = np.array(total_data)\n    if condition is 'train' :\n        total_label = np.array(total_label)\n    \n    if condition is 'train' :\n        return total_data, total_label\n    elif condition is 'test' :\n        return total_data","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:37:00.674734Z","iopub.execute_input":"2022-04-24T13:37:00.675401Z","iopub.status.idle":"2022-04-24T13:37:00.739262Z","shell.execute_reply.started":"2022-04-24T13:37:00.675361Z","shell.execute_reply":"2022-04-24T13:37:00.738617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, train_label = feature_extractor(train_time_dictionary_window,train_frequent_dictionary_window,condition='train')\ntest_data = feature_extractor(test_time_dictionary_window,test_frequent_dictionary_window,condition='test')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:37:00.74171Z","iopub.execute_input":"2022-04-24T13:37:00.741907Z","iopub.status.idle":"2022-04-24T13:45:15.765233Z","shell.execute_reply.started":"2022-04-24T13:37:00.741883Z","shell.execute_reply":"2022-04-24T13:45:15.764337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data)\nprint(test_data)\n\nlen(train_time_dictionary_window.keys())","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:45:15.766997Z","iopub.execute_input":"2022-04-24T13:45:15.767214Z","iopub.status.idle":"2022-04-24T13:45:15.776479Z","shell.execute_reply.started":"2022-04-24T13:45:15.767174Z","shell.execute_reply":"2022-04-24T13:45:15.775736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------------------------\n# [Empty Module #2] Data Normalization\n# -------------------------------------\n\n# -------------------------------------\n# Data Normalization\n# -------------------------------------\n# 목적: 앞서 구축한 train,test 셋에 대한 Feature를 정규화한다.\n# 입력인자: train 셋에서 추출된 Feature, test 셋에서 추출된 Feature\n# 출력인자: 정규화된 Feature Vector\n# -------------------------------------\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(train_data)\ntrain_norm_data = scaler.transform(train_data)\ntest_norm_data = scaler.transform(test_data)\n# ------------------------------------------------------------\n# 구현 가이드라인 \n# ------------------------------------------------------------\n# sklearn에서 제공하는 MinMaxScaler를 사용해 데이터 정규화를 진행한다.\n# (MinMaxScaler가 아닌 다른 정규화를 사용할 수 있다.)\n# ------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:45:15.777752Z","iopub.execute_input":"2022-04-24T13:45:15.778539Z","iopub.status.idle":"2022-04-24T13:45:16.715074Z","shell.execute_reply.started":"2022-04-24T13:45:15.7785Z","shell.execute_reply":"2022-04-24T13:45:16.714347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------------------------\n# [Empty Module #3] RandomForest를 이용한 분류\n# -------------------------------------\n\n# -------------------------------------\n# SVC를 이용한 분류\n# -------------------------------------\n# 목적: 앞서 완성한 train/test Feature를 RandomForest를 이용해 분류한다.\n# 입력인자: Feature vector(train/test)\n# 출력인자: 분류결과\n# -------------------------------------\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=1000, max_features='log2', random_state=42)\nrf.fit(train_norm_data, train_label)\ny_pred = rf.predict(test_norm_data)\n\n# ------------------------------------------------------------\n# 구현 가이드라인 \n# ------------------------------------------------------------\n# sklearn에서 제공하는 RandomForest를 사용해 데이터 정규화를 진행한다.\n# (RandomForest를가 아닌 다른 분류모델을 사용할 수 있다.)\n# ------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:47:01.537089Z","iopub.execute_input":"2022-04-24T13:47:01.537774Z","iopub.status.idle":"2022-04-24T13:47:02.732945Z","shell.execute_reply.started":"2022-04-24T13:47:01.537734Z","shell.execute_reply":"2022-04-24T13:47:02.732243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_csv = pd.read_csv('../input/2022-ml-w7p1/submit.csv')\nsubmit_csv['Label'] = y_pred\nsubmit_csv['Label'] = submit_csv['Label'].astype(\"int\")\nsubmit_csv.to_csv(\"./result-rf.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T13:47:05.981835Z","iopub.execute_input":"2022-04-24T13:47:05.982111Z","iopub.status.idle":"2022-04-24T13:47:06.00144Z","shell.execute_reply.started":"2022-04-24T13:47:05.982078Z","shell.execute_reply":"2022-04-24T13:47:06.000744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}